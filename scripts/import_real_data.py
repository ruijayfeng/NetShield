#!/usr/bin/env python3
"""
ç¤ºä¾‹è„šæœ¬ï¼šå°†çœŸå®æ•°æ®å¯¼å…¥åˆ°ç½‘ç»œå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿè¿›è¡Œåˆ†æ

ä½¿ç”¨æ–¹æ³•:
python scripts/import_real_data.py --network-file data/real/network.csv --data-file data/real/monitoring.csv
"""

import argparse
import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import networkx as nx

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from main import NetworkAnalysisSystem
import asyncio


def create_sample_network_file(filepath: str):
    """åˆ›å»ºç¤ºä¾‹ç½‘ç»œæ‹“æ‰‘æ–‡ä»¶"""
    print(f"åˆ›å»ºç¤ºä¾‹ç½‘ç»œæ–‡ä»¶: {filepath}")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ç½‘ç»œæ‹“æ‰‘
    edges = [
        {'source': 'node_1', 'target': 'node_2', 'weight': 0.8},
        {'source': 'node_1', 'target': 'node_3', 'weight': 0.9},
        {'source': 'node_2', 'target': 'node_3', 'weight': 1.2},
        {'source': 'node_2', 'target': 'node_4', 'weight': 0.7},
        {'source': 'node_3', 'target': 'node_4', 'weight': 1.0},
        {'source': 'node_3', 'target': 'node_5', 'weight': 0.6},
        {'source': 'node_4', 'target': 'node_5', 'weight': 0.9},
    ]
    
    df = pd.DataFrame(edges)
    df.to_csv(filepath, index=False)
    print(f"âœ… ç½‘ç»œæ–‡ä»¶å·²åˆ›å»º: {len(edges)} æ¡è¾¹")


def create_sample_monitoring_data(filepath: str, duration_hours: int = 24):
    """åˆ›å»ºç¤ºä¾‹ç›‘æ§æ•°æ®æ–‡ä»¶"""
    print(f"åˆ›å»ºç¤ºä¾‹ç›‘æ§æ•°æ®æ–‡ä»¶: {filepath}")
    
    # ç”Ÿæˆæ—¶é—´åºåˆ—
    start_time = datetime.now() - timedelta(hours=duration_hours)
    timestamps = [start_time + timedelta(minutes=i) for i in range(duration_hours * 60)]
    
    data = []
    for i, timestamp in enumerate(timestamps):
        # åŸºç¡€æ¨¡å¼ + éšæœºå™ªå£°
        base_traffic = 100 + 20 * np.sin(2 * np.pi * i / (24 * 60)) + np.random.normal(0, 5)
        base_latency = 25 + 5 * np.sin(2 * np.pi * i / (12 * 60)) + np.random.normal(0, 2)
        base_cpu = 0.3 + 0.2 * np.sin(2 * np.pi * i / (24 * 60)) + np.random.normal(0, 0.05)
        base_memory = 0.5 + 0.1 * np.sin(2 * np.pi * i / (48 * 60)) + np.random.normal(0, 0.02)
        
        # æ³¨å…¥ä¸€äº›å¼‚å¸¸
        is_anomaly = False
        if np.random.random() < 0.03:  # 3% å¼‚å¸¸ç‡
            is_anomaly = True
            base_traffic *= np.random.uniform(2, 4)  # æµé‡å¼‚å¸¸
            base_latency *= np.random.uniform(2, 3)  # å»¶è¿Ÿå¼‚å¸¸
            base_cpu = min(1.0, base_cpu * np.random.uniform(1.5, 2.5))  # CPUå¼‚å¸¸
        
        record = {
            'timestamp': timestamp.strftime('%Y-%m-%d %H:%M:%S'),
            'traffic': max(0, base_traffic),
            'latency': max(0, base_latency),
            'packet_loss': max(0, np.random.exponential(0.001)),
            'cpu_usage': np.clip(base_cpu, 0, 1),
            'memory_usage': np.clip(base_memory, 0, 1),
            'node_id': 'node_1',
            'is_anomaly': is_anomaly,
            'anomaly_score': 0.8 if is_anomaly else 0.0
        }
        data.append(record)
    
    df = pd.DataFrame(data)
    df.to_csv(filepath, index=False)
    print(f"âœ… ç›‘æ§æ•°æ®æ–‡ä»¶å·²åˆ›å»º: {len(data)} æ¡è®°å½•, å¼‚å¸¸ç‡: {df['is_anomaly'].mean():.3f}")
    return df


def validate_data_files(network_file: str, data_file: str):
    """éªŒè¯æ•°æ®æ–‡ä»¶æ ¼å¼"""
    print("éªŒè¯æ•°æ®æ–‡ä»¶æ ¼å¼...")
    
    # éªŒè¯ç½‘ç»œæ–‡ä»¶
    if network_file and os.path.exists(network_file):
        try:
            network_df = pd.read_csv(network_file)
            required_cols = ['source', 'target']
            missing_cols = [col for col in required_cols if col not in network_df.columns]
            if missing_cols:
                print(f"âŒ ç½‘ç»œæ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
                return False
            print(f"âœ… ç½‘ç»œæ–‡ä»¶æ ¼å¼æ­£ç¡®: {len(network_df)} æ¡è¾¹")
        except Exception as e:
            print(f"âŒ ç½‘ç»œæ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            return False
    
    # éªŒè¯ç›‘æ§æ•°æ®æ–‡ä»¶
    if not os.path.exists(data_file):
        print(f"âŒ ç›‘æ§æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return False
    
    try:
        data_df = pd.read_csv(data_file)
        required_cols = ['timestamp']
        missing_cols = [col for col in required_cols if col not in data_df.columns]
        if missing_cols:
            print(f"âŒ ç›‘æ§æ•°æ®æ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
            return False
        
        # æ£€æŸ¥æ•°å€¼ç‰¹å¾åˆ—
        numeric_cols = data_df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) < 1:
            print("âŒ ç›‘æ§æ•°æ®æ–‡ä»¶è‡³å°‘éœ€è¦ä¸€ä¸ªæ•°å€¼ç‰¹å¾åˆ—")
            return False
            
        print(f"âœ… ç›‘æ§æ•°æ®æ–‡ä»¶æ ¼å¼æ­£ç¡®: {len(data_df)} æ¡è®°å½•, {len(numeric_cols)} ä¸ªæ•°å€¼ç‰¹å¾")
        return True
        
    except Exception as e:
        print(f"âŒ ç›‘æ§æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        return False


async def run_analysis_with_real_data(network_file: str, data_file: str, output_dir: str):
    """ä½¿ç”¨çœŸå®æ•°æ®è¿è¡Œå®Œæ•´åˆ†æ"""
    print("å¼€å§‹ä½¿ç”¨çœŸå®æ•°æ®è¿›è¡Œåˆ†æ...")
    
    try:
        # åˆ›å»ºåˆ†æç³»ç»Ÿ
        system = NetworkAnalysisSystem()
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        print("åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶...")
        system.initialize_system()
        
        # åŠ è½½çœŸå®æ•°æ®
        print("åŠ è½½çœŸå®æ•°æ®...")
        system.generate_network_and_data(network_file, data_file)
        
        # è®­ç»ƒæ¨¡å‹
        print("è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹...")
        training_stats = system.train_anomaly_detection()
        
        # æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
        print("æ‰§è¡Œå¼‚å¸¸æ£€æµ‹...")
        detection_results = await system.perform_anomaly_detection()
        
        # æ‰§è¡Œçº§è”å¤±æ•ˆåˆ†æ
        print("æ‰§è¡Œçº§è”å¤±æ•ˆåˆ†æ...")
        cascade_results = await system.perform_cascading_failure_analysis()
        
        # ç”Ÿæˆè§£é‡Š
        print("ç”Ÿæˆå¯è§£é‡Šæ€§åˆ†æ...")
        try:
            explanations, exp_report = system.generate_explanations(5)
            print("âœ… å¯è§£é‡Šæ€§åˆ†æå®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ å¯è§£é‡Šæ€§åˆ†æå¤±è´¥: {e}")
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        print("ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
        report = system.generate_comprehensive_report()
        print("\n" + "="*80)
        print(report)
        print("="*80)
        
        # ä¿å­˜ç»“æœ
        print(f"ä¿å­˜åˆ†æç»“æœåˆ° {output_dir}...")
        system.save_results(output_dir)
        
        # æ˜¾ç¤ºå…³é”®ç»“æœ
        summary = detection_results.get('summary', {})
        print(f"\nğŸ“Š åˆ†æç»“æœæ‘˜è¦:")
        print(f"   å¼‚å¸¸æ£€æµ‹: {summary.get('predicted_anomalies', 0)} ä¸ªå¼‚å¸¸")
        print(f"   å¼‚å¸¸ç‡: {summary.get('anomaly_rate', 0):.1%}")
        
        robustness_metrics = cascade_results.get('robustness_metrics', {})
        robustness_score = robustness_metrics.get('overall_robustness_score', 0)
        print(f"   ç½‘ç»œé²æ£’æ€§: {robustness_score:.3f}")
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {output_dir}")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹å¤±è´¥: {e}")
        raise


def main():
    parser = argparse.ArgumentParser(
        description="å¯¼å…¥çœŸå®æ•°æ®è¿›è¡Œç½‘ç»œå¼‚å¸¸æ£€æµ‹å’Œçº§è”å¤±æ•ˆåˆ†æ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä½¿ç”¨çœŸå®æ•°æ®æ–‡ä»¶
  python scripts/import_real_data.py --network-file data/network.csv --data-file data/monitoring.csv
  
  # ç”Ÿæˆç¤ºä¾‹æ•°æ®å¹¶åˆ†æ
  python scripts/import_real_data.py --create-sample --output output/
  
  # ä»…éªŒè¯æ•°æ®æ ¼å¼
  python scripts/import_real_data.py --data-file data/monitoring.csv --validate-only
        """
    )
    
    parser.add_argument(
        '--network-file',
        help='ç½‘ç»œæ‹“æ‰‘æ–‡ä»¶è·¯å¾„ (CSVæ ¼å¼: source,target,weight)'
    )
    
    parser.add_argument(
        '--data-file',
        help='ç›‘æ§æ•°æ®æ–‡ä»¶è·¯å¾„ (CSVæ ¼å¼ï¼Œå¿…é¡»åŒ…å«timestampåˆ—)'
    )
    
    parser.add_argument(
        '--output',
        default='output_real_data',
        help='è¾“å‡ºç›®å½• (é»˜è®¤: output_real_data)'
    )
    
    parser.add_argument(
        '--create-sample',
        action='store_true',
        help='åˆ›å»ºç¤ºä¾‹æ•°æ®æ–‡ä»¶ç”¨äºæµ‹è¯•'
    )
    
    parser.add_argument(
        '--validate-only',
        action='store_true',
        help='ä»…éªŒè¯æ•°æ®æ–‡ä»¶æ ¼å¼ï¼Œä¸æ‰§è¡Œåˆ†æ'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    if args.create_sample:
        os.makedirs('data/sample', exist_ok=True)
        network_file = 'data/sample/network.csv'
        data_file = 'data/sample/monitoring.csv'
        
        create_sample_network_file(network_file)
        create_sample_monitoring_data(data_file)
        
        print(f"\nç¤ºä¾‹æ•°æ®æ–‡ä»¶å·²åˆ›å»º:")
        print(f"  ç½‘ç»œæ–‡ä»¶: {network_file}")
        print(f"  ç›‘æ§æ•°æ®: {data_file}")
        print(f"\nç°åœ¨å¯ä»¥è¿è¡Œ:")
        print(f"  python scripts/import_real_data.py --network-file {network_file} --data-file {data_file}")
        return
    
    # æ£€æŸ¥å¿…éœ€å‚æ•°
    if not args.data_file:
        print("âŒ è¯·æŒ‡å®šç›‘æ§æ•°æ®æ–‡ä»¶ (--data-file)")
        parser.print_help()
        return
    
    # éªŒè¯æ•°æ®æ–‡ä»¶
    if not validate_data_files(args.network_file, args.data_file):
        print("âŒ æ•°æ®æ–‡ä»¶éªŒè¯å¤±è´¥")
        return
    
    if args.validate_only:
        print("âœ… æ•°æ®æ–‡ä»¶éªŒè¯é€šè¿‡")
        return
    
    # æ‰§è¡Œåˆ†æ
    try:
        asyncio.run(run_analysis_with_real_data(
            args.network_file, 
            args.data_file, 
            args.output
        ))
    except KeyboardInterrupt:
        print("\nâŒ åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")


if __name__ == "__main__":
    main()