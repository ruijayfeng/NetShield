"""
Streamlit dashboard for network anomaly detection and cascading failure analysis.
"""

import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
import numpy as np
import networkx as nx
from datetime import datetime, timedelta
import asyncio
import time
import yaml
import os
import sys
import requests
import json
from typing import Dict, List, Any, Optional

# Add src to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from src.data.generators.network_generator import NetworkGenerator, NetworkConfig, RealNetworkInterface
from src.data.generators.data_generator import NetworkDataGenerator, DataConfig
from src.models.anomaly_detection.detectors import NetworkAnomalyAnalyzer, AnomalyDetectionConfig
from src.models.cascading.failure_analyzer import CascadingFailureAnalyzer, CascadingFailureConfig
from src.models.explainable.shap_explainer import NetworkAnomalyExplainer, ExplainabilityConfig
from src.alerts.alert_manager import AlertManager, AlertLevel, AlertCategory


class NetworkDashboard:
    """Main dashboard class for the network monitoring system"""
    
    def __init__(self):
        self.setup_page_config()
        self.initialize_session_state()
        
    def setup_page_config(self):
        """Configure Streamlit page"""
        st.set_page_config(
            page_title="ç½‘ç»œå¼‚å¸¸æ£€æµ‹ä¸çº§è”å¤±æ•ˆåˆ†æç³»ç»Ÿ",
            page_icon="ğŸ•¸ï¸",
            layout="wide",
            initial_sidebar_state="expanded",
            menu_items={
                'Get Help': 'https://github.com/your-repo/fenxi',
                'Report a bug': 'https://github.com/your-repo/fenxi/issues',
                'About': "å¤æ‚ç½‘ç»œå¼‚å¸¸è¡Œä¸ºæ£€æµ‹ä¸çº§è”å¤±æ•ˆåˆ†æç³»ç»Ÿ"
            }
        )
        
        # Custom CSS
        st.markdown("""
        <style>
        .metric-card {
            background-color: #f0f2f6;
            padding: 1rem;
            border-radius: 0.5rem;
            border-left: 4px solid #1f77b4;
        }
        .alert-critical {
            background-color: #ffebee;
            border-left: 4px solid #f44336;
            padding: 1rem;
            border-radius: 0.5rem;
            margin: 0.5rem 0;
        }
        .alert-warning {
            background-color: #fff8e1;
            border-left: 4px solid #ff9800;
            padding: 1rem;
            border-radius: 0.5rem;
            margin: 0.5rem 0;
        }
        .alert-info {
            background-color: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: 1rem;
            border-radius: 0.5rem;
            margin: 0.5rem 0;
        }
        </style>
        """, unsafe_allow_html=True)
    
    def initialize_session_state(self):
        """Initialize Streamlit session state"""
        if 'network' not in st.session_state:
            st.session_state.network = None
        
        if 'data' not in st.session_state:
            st.session_state.data = None
        
        if 'anomaly_analyzer' not in st.session_state:
            st.session_state.anomaly_analyzer = None
        
        if 'cascade_analyzer' not in st.session_state:
            st.session_state.cascade_analyzer = None
        
        if 'explainer' not in st.session_state:
            st.session_state.explainer = None
        
        if 'alert_manager' not in st.session_state:
            st.session_state.alert_manager = AlertManager()
        
        if 'last_update' not in st.session_state:
            st.session_state.last_update = datetime.now()
        
        if 'analysis_results' not in st.session_state:
            st.session_state.analysis_results = {}

        # AIåˆ†æç›¸å…³çŠ¶æ€
        if 'ai_chat_history' not in st.session_state:
            st.session_state.ai_chat_history = []

        if 'ai_summary' not in st.session_state:
            st.session_state.ai_summary = None

        if 'ai_api_key' not in st.session_state:
            st.session_state.ai_api_key = "ä¸è®¸å·çœ‹æˆ‘çš„å°å¯†é’¥ï¼"
    
    def render_dashboard(self):
        """Render the main dashboard"""
        st.title("ğŸ•¸ï¸ å¤æ‚ç½‘ç»œå¼‚å¸¸è¡Œä¸ºæ£€æµ‹ä¸çº§è”å¤±æ•ˆåˆ†æç³»ç»Ÿ")
        st.markdown("---")
        
        # Sidebar configuration
        self.render_sidebar()
        
        # Main content based on selected page
        page = st.session_state.get('current_page', 'overview')
        
        if page == 'overview':
            self.render_overview_page()
        elif page == 'network':
            self.render_network_page()
        elif page == 'anomaly':
            self.render_anomaly_page()
        elif page == 'cascading':
            self.render_cascading_page()
        elif page == 'explainability':
            self.render_explainability_page()
        elif page == 'ai_analysis':
            self.render_ai_analysis_page()
        elif page == 'alerts':
            self.render_alerts_page()
        else:
            self.render_overview_page()
    
    def render_sidebar(self):
        """Render sidebar with navigation and controls"""
        with st.sidebar:
            st.header("âš™ï¸ ç³»ç»Ÿæ§åˆ¶")
            
            # Page navigation
            st.subheader("ğŸ“„ é¡µé¢å¯¼èˆª")
            pages = {
                'overview': 'ğŸ“Š ç³»ç»Ÿæ¦‚è§ˆ',
                'network': 'ğŸ”— ç½‘ç»œæ‹“æ‰‘',
                'anomaly': 'ğŸš¨ å¼‚å¸¸æ£€æµ‹',
                'cascading': 'âš¡ çº§è”å¤±æ•ˆ',
                'explainability': 'ğŸ” å¯è§£é‡Šæ€§',
                'ai_analysis': 'ğŸ¤– AIå¤§æ¨¡å‹åˆ†æ',
                'alerts': 'ğŸ“¢ å‘Šè­¦ç®¡ç†'
            }
            
            selected_page = st.selectbox(
                "é€‰æ‹©é¡µé¢",
                list(pages.keys()),
                format_func=lambda x: pages[x],
                key='current_page'
            )
            
            st.markdown("---")
            
            # System configuration
            st.subheader("ğŸ”§ ç³»ç»Ÿé…ç½®")
            
            # Data source selection
            st.subheader("ğŸ“‚ æ•°æ®æºé€‰æ‹©")
            data_source = st.radio(
                "é€‰æ‹©æ•°æ®æº",
                ["ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®", "ä¸Šä¼ çœŸå®æ•°æ®"],
                key="data_source_option"
            )
            
            if data_source == "ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®":
                # Network configuration
                with st.expander("ç½‘ç»œé…ç½®", expanded=True):
                    node_count = st.slider("èŠ‚ç‚¹æ•°é‡", 10, 100, 50)
                    network_type = st.selectbox(
                        "ç½‘ç»œç±»å‹",
                        ['small_world', 'scale_free', 'erdos_renyi'],
                        format_func=lambda x: {
                            'small_world': 'å°ä¸–ç•Œç½‘ç»œ',
                            'scale_free': 'æ— æ ‡åº¦ç½‘ç»œ',
                            'erdos_renyi': 'éšæœºç½‘ç»œ'
                        }[x]
                    )
                
                # Data configuration
                with st.expander("æ•°æ®é…ç½®"):
                    time_steps = st.slider("æ—¶é—´æ­¥æ•°", 100, 2000, 500)
                    anomaly_ratio = st.slider("å¼‚å¸¸æ¯”ä¾‹", 0.01, 0.2, 0.05)
                    
                # Generate or update data
                if st.button("ğŸ”„ ç”Ÿæˆ/æ›´æ–°æ•°æ®", type="primary"):
                    self.generate_data(node_count, network_type, time_steps, anomaly_ratio)
            
            else:  # Upload real data
                with st.expander("ä¸Šä¼ çœŸå®æ•°æ®", expanded=True):
                    st.markdown("**ç½‘ç»œæ‹“æ‰‘æ–‡ä»¶** (å¯é€‰)")
                    network_file = st.file_uploader(
                        "ä¸Šä¼ ç½‘ç»œæ‹“æ‰‘æ–‡ä»¶",
                        type=['csv', 'json', 'gml', 'graphml'],
                        help="CSVæ ¼å¼éœ€åŒ…å«source,targetåˆ—ï¼›JSONæ ¼å¼éœ€åŒ…å«nodeså’Œedges"
                    )
                    
                    st.markdown("**ç›‘æ§æ•°æ®æ–‡ä»¶** (å¿…éœ€)")
                    data_file = st.file_uploader(
                        "ä¸Šä¼ ç›‘æ§æ•°æ®æ–‡ä»¶",
                        type=['csv', 'json', 'parquet'],
                        help="å¿…é¡»åŒ…å«timestampåˆ—å’Œè‡³å°‘ä¸€ä¸ªæ•°å€¼ç‰¹å¾åˆ—"
                    )
                    
                    if st.button("ğŸ“¤ åŠ è½½çœŸå®æ•°æ®", type="primary"):
                        self.load_real_data(network_file, data_file)
            
            st.markdown("---")
            
            # System status
            st.subheader("ğŸ“ˆ ç³»ç»ŸçŠ¶æ€")
            
            if st.session_state.network is not None:
                st.success(f"âœ… ç½‘ç»œå·²åŠ è½½ ({st.session_state.network.number_of_nodes()} èŠ‚ç‚¹)")
            else:
                st.warning("âš ï¸ æœªåŠ è½½ç½‘ç»œ")
            
            if st.session_state.data is not None:
                st.success(f"âœ… æ•°æ®å·²ç”Ÿæˆ ({len(st.session_state.data)} æ¡è®°å½•)")
            else:
                st.warning("âš ï¸ æœªç”Ÿæˆæ•°æ®")
            
            # Auto-refresh toggle
            auto_refresh = st.checkbox("ğŸ”„ è‡ªåŠ¨åˆ·æ–°", value=False)
            if auto_refresh:
                st.rerun()
    
    def generate_data(self, node_count: int, network_type: str, 
                     time_steps: int, anomaly_ratio: float):
        """Generate network and data"""
        with st.spinner("æ­£åœ¨ç”Ÿæˆç½‘ç»œå’Œæ•°æ®..."):
            try:
                # Generate network
                network_config = NetworkConfig(
                    node_count=node_count,
                    network_type=network_type
                )
                
                network_gen = NetworkGenerator(network_config)
                st.session_state.network = network_gen.generate_network()
                
                # Generate data
                data_config = DataConfig(
                    time_steps=time_steps,
                    anomaly_ratio=anomaly_ratio
                )
                
                data_gen = NetworkDataGenerator(data_config)
                node_id = list(st.session_state.network.nodes())[0]
                st.session_state.data = data_gen.generate_node_timeseries(
                    st.session_state.network, node_id
                )
                
                # Train models
                self.train_models()
                
                st.session_state.last_update = datetime.now()
                st.success("âœ… æ•°æ®ç”Ÿæˆå®Œæˆï¼")
                
            except Exception as e:
                st.error(f"âŒ æ•°æ®ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    def load_real_data(self, network_file, data_file):
        """Load real network and monitoring data"""
        if data_file is None:
            st.error("âŒ è¯·ä¸Šä¼ ç›‘æ§æ•°æ®æ–‡ä»¶")
            return
            
        try:
            with st.spinner("æ­£åœ¨åŠ è½½çœŸå®æ•°æ®..."):
                # Load network data if provided
                if network_file is not None:
                    # Save uploaded file temporarily
                    import tempfile
                    with tempfile.NamedTemporaryFile(delete=False, suffix=f".{network_file.name.split('.')[-1]}") as tmp:
                        tmp.write(network_file.read())
                        tmp_network_path = tmp.name
                    
                    # Load network using RealNetworkInterface
                    interface = RealNetworkInterface()
                    st.session_state.network = interface.load_from_file(tmp_network_path)
                    
                    # Clean up temp file
                    os.unlink(tmp_network_path)
                    
                    st.success(f"âœ… ç½‘ç»œæ‹“æ‰‘å·²åŠ è½½: {st.session_state.network.number_of_nodes()} èŠ‚ç‚¹, "
                             f"{st.session_state.network.number_of_edges()} æ¡è¾¹")
                else:
                    # Generate a default network if none provided
                    network_gen = NetworkGenerator(NetworkConfig(node_count=20))
                    st.session_state.network = network_gen.generate_network()
                    st.info("â„¹ï¸ æœªæä¾›ç½‘ç»œæ–‡ä»¶ï¼Œå·²ç”Ÿæˆé»˜è®¤ç½‘ç»œæ‹“æ‰‘")
                
                # Load monitoring data
                import tempfile
                with tempfile.NamedTemporaryFile(delete=False, suffix=f".{data_file.name.split('.')[-1]}") as tmp:
                    tmp.write(data_file.read())
                    tmp_data_path = tmp.name
                
                # Load data using NetworkDataGenerator
                data_gen = NetworkDataGenerator()
                st.session_state.data = data_gen.load_real_data(tmp_data_path)
                
                # Clean up temp file
                os.unlink(tmp_data_path)
                
                # Validate data
                validation = data_gen.validate_data_quality(st.session_state.data)
                if not validation['is_valid']:
                    st.warning(f"âš ï¸ æ•°æ®è´¨é‡é—®é¢˜: {', '.join(validation['issues'])}")
                else:
                    st.success("âœ… æ•°æ®è´¨é‡éªŒè¯é€šè¿‡")
                
                # Display data info
                st.info(f"ğŸ“Š ç›‘æ§æ•°æ®å·²åŠ è½½: {len(st.session_state.data)} æ¡è®°å½•, "
                       f"{len(st.session_state.data.columns)} ä¸ªç‰¹å¾")
                
                # Train models
                self.train_models()
                
                st.session_state.last_update = datetime.now()
                st.success("âœ… çœŸå®æ•°æ®åŠ è½½å®Œæˆï¼")
                
        except Exception as e:
            st.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
    
    def train_models(self):
        """Train anomaly detection and other models"""
        if st.session_state.data is None:
            return
        
        try:
            # Train anomaly detection model
            ad_config = AnomalyDetectionConfig()
            st.session_state.anomaly_analyzer = NetworkAnomalyAnalyzer(ad_config)
            
            # Split data for training
            data = st.session_state.data
            split_idx = int(0.7 * len(data))
            train_data = data[:split_idx]
            
            st.session_state.anomaly_analyzer.train(train_data)
            
            # Initialize other analyzers
            cf_config = CascadingFailureConfig(num_simulations=20)  # Reduced for demo
            st.session_state.cascade_analyzer = CascadingFailureAnalyzer(cf_config)
            
            # Initialize explainer
            if st.session_state.anomaly_analyzer.ensemble.detectors:
                isolation_forest = st.session_state.anomaly_analyzer.ensemble.detectors['isolation_forest']
                exp_config = ExplainabilityConfig()
                st.session_state.explainer = NetworkAnomalyExplainer(isolation_forest.model, exp_config)
                
                # Setup explainer with background data
                X_train = st.session_state.anomaly_analyzer.prepare_features(train_data)
                st.session_state.explainer.setup_explainer(
                    X_train, st.session_state.anomaly_analyzer.feature_columns
                )
            
        except Exception as e:
            st.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}")

    def call_zhipu_ai(self, messages: List[Dict], system_prompt: str = None):
        """è°ƒç”¨æ™ºè°±AI API"""
        try:
            # æ„å»ºè¯·æ±‚æ¶ˆæ¯
            if system_prompt:
                # å°†ç³»ç»Ÿæç¤ºè¯ä½œä¸ºç¬¬ä¸€æ¡æ¶ˆæ¯
                all_messages = [{"role": "system", "content": system_prompt}] + messages
            else:
                all_messages = messages

            payload = {
                "model": "glm-4-flash",  # ä½¿ç”¨æ€§ä»·æ¯”é«˜çš„æ¨¡å‹
                "messages": all_messages,
                "thinking": {"type": "enabled"},
                "max_tokens": 4096,
                "temperature": 0.6
            }

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {st.session_state.ai_api_key}"
            }

            response = requests.post(
                "https://open.bigmodel.cn/api/paas/v4/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            else:
                return f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}"

        except Exception as e:
            return f"è°ƒç”¨AIæœåŠ¡æ—¶å‡ºé”™: {str(e)}"

    def get_system_context(self):
        """è·å–å½“å‰ç³»ç»ŸçŠ¶æ€ä½œä¸ºAIä¸Šä¸‹æ–‡"""
        context = {}

        if st.session_state.network:
            context["network"] = {
                "node_count": st.session_state.network.number_of_nodes(),
                "edge_count": st.session_state.network.number_of_edges(),
                "density": nx.density(st.session_state.network),
                "is_connected": nx.is_connected(st.session_state.network)
            }

        if st.session_state.data is not None:
            data = st.session_state.data
            context["data"] = {
                "total_points": len(data),
                "anomaly_count": int(data['is_anomaly'].sum()) if 'is_anomaly' in data.columns else 0,
                "anomaly_rate": data['is_anomaly'].mean() if 'is_anomaly' in data.columns else 0,
                "features": [col for col in data.columns if col not in ['timestamp', 'is_anomaly', 'anomaly_score']]
            }

        if st.session_state.alert_manager:
            stats = st.session_state.alert_manager.get_alert_statistics()
            context["alerts"] = stats

        return context
    
    def render_overview_page(self):
        """Render system overview page"""
        st.header("ğŸ“Š ç³»ç»Ÿæ¦‚è§ˆ")
        
        if st.session_state.network is None or st.session_state.data is None:
            st.info("ğŸ‘† è¯·å…ˆåœ¨ä¾§è¾¹æ ç”Ÿæˆç½‘ç»œå’Œæ•°æ®")
            return
        
        # Key metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "ç½‘ç»œèŠ‚ç‚¹æ•°",
                st.session_state.network.number_of_nodes(),
                delta=None
            )
        
        with col2:
            st.metric(
                "ç½‘ç»œè¾¹æ•°",
                st.session_state.network.number_of_edges(),
                delta=None
            )
        
        with col3:
            anomaly_count = int(st.session_state.data['is_anomaly'].sum())
            st.metric(
                "æ£€æµ‹åˆ°å¼‚å¸¸",
                anomaly_count,
                delta=f"{anomaly_count/len(st.session_state.data):.1%}"
            )
        
        with col4:
            active_alerts = len(st.session_state.alert_manager.get_active_alerts())
            st.metric(
                "æ´»è·ƒå‘Šè­¦",
                active_alerts,
                delta=None
            )
        
        st.markdown("---")
        
        # Recent data visualization
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“ˆ æœ€è¿‘æ—¶åºæ•°æ®")
            self.plot_timeseries_overview()
        
        with col2:
            st.subheader("ğŸ”— ç½‘ç»œæ‹“æ‰‘æ¦‚è§ˆ")
            self.plot_network_overview()
        
        # System status
        st.markdown("---")
        st.subheader("ğŸ“Š ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # Anomaly detection performance
            if st.session_state.anomaly_analyzer:
                st.info("âœ… å¼‚å¸¸æ£€æµ‹æ¨¡å‹å·²è®­ç»ƒ")
                # Show some mock metrics
                st.metric("æ£€æµ‹å‡†ç¡®ç‡", "87.3%", "2.1%")
        
        with col2:
            # Cascading failure analysis
            if st.session_state.cascade_analyzer:
                st.info("âœ… çº§è”å¤±æ•ˆåˆ†æå°±ç»ª")
                st.metric("ç½‘ç»œé²æ£’æ€§", "0.73", "-0.05")
        
        with col3:
            # Explainability
            if st.session_state.explainer:
                st.info("âœ… å¯è§£é‡Šæ€§åˆ†æå°±ç»ª")
                st.metric("è§£é‡Šç½®ä¿¡åº¦", "91.2%", "1.8%")
    
    def plot_timeseries_overview(self):
        """Plot overview of time series data"""
        data = st.session_state.data
        
        # Sample data for display (last 100 points)
        display_data = data.tail(100).copy()
        
        fig = go.Figure()
        
        # Detect traffic column name
        traffic_col = None
        for col in ['traffic_mbps', 'traffic', 'throughput_mbps', 'network_throughput']:
            if col in display_data.columns:
                traffic_col = col
                break
        
        if traffic_col:
            # Plot traffic
            fig.add_trace(go.Scatter(
                x=display_data.index,
                y=display_data[traffic_col],
                mode='lines',
                name='ç½‘ç»œæµé‡',
                line=dict(color='blue')
            ))
            
            # Highlight anomalies
            anomaly_data = display_data[display_data['is_anomaly'] == True]
            if not anomaly_data.empty:
                fig.add_trace(go.Scatter(
                    x=anomaly_data.index,
                    y=anomaly_data[traffic_col],
                    mode='markers',
                    name='å¼‚å¸¸ç‚¹',
                    marker=dict(color='red', size=8, symbol='circle-open')
                ))
        
        fig.update_layout(
            title="ç½‘ç»œæµé‡æ—¶åºå›¾",
            xaxis_title="æ—¶é—´ç‚¹",
            yaxis_title="æµé‡å€¼",
            height=300,
            showlegend=True
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    def plot_network_overview(self):
        """Plot network topology overview"""
        network = st.session_state.network
        
        # Use spring layout for positioning
        pos = nx.spring_layout(network, k=1, iterations=50)
        
        # Prepare node data
        node_x = [pos[node][0] for node in network.nodes()]
        node_y = [pos[node][1] for node in network.nodes()]
        
        # Node colors based on degree
        degrees = dict(network.degree())
        max_degree = max(degrees.values())
        node_colors = [degrees[node]/max_degree for node in network.nodes()]
        
        # Prepare edge data
        edge_x = []
        edge_y = []
        for edge in network.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
        
        # Create figure
        fig = go.Figure()
        
        # Add edges
        fig.add_trace(go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=1, color='lightgray'),
            hoverinfo='none',
            mode='lines',
            showlegend=False
        ))
        
        # Add nodes
        fig.add_trace(go.Scatter(
            x=node_x, y=node_y,
            mode='markers',
            marker=dict(
                size=8,
                color=node_colors,
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(title="èŠ‚ç‚¹åº¦æ•°")
            ),
            text=[f"èŠ‚ç‚¹ {node}<br>åº¦æ•°: {degrees[node]}" for node in network.nodes()],
            hoverinfo='text',
            showlegend=False
        ))
        
        fig.update_layout(
            title="ç½‘ç»œæ‹“æ‰‘ç»“æ„",
            showlegend=False,
            hovermode='closest',
            margin=dict(b=20,l=5,r=5,t=40),
            annotations=[
                dict(
                    text=f"èŠ‚ç‚¹: {network.number_of_nodes()}, è¾¹: {network.number_of_edges()}",
                    showarrow=False,
                    xref="paper", yref="paper",
                    x=0.005, y=-0.002,
                    xanchor='left', yanchor='bottom',
                    font=dict(color='gray', size=12)
                )
            ],
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            height=300
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    def render_network_page(self):
        """Render network topology analysis page"""
        st.header("ğŸ”— ç½‘ç»œæ‹“æ‰‘åˆ†æ")
        
        if st.session_state.network is None:
            st.info("è¯·å…ˆç”Ÿæˆç½‘ç»œæ•°æ®")
            return
        
        network = st.session_state.network
        
        # Network statistics
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.subheader("åŸºæœ¬ç»Ÿè®¡")
            st.metric("èŠ‚ç‚¹æ•°", network.number_of_nodes())
            st.metric("è¾¹æ•°", network.number_of_edges())
            st.metric("ç½‘ç»œå¯†åº¦", f"{nx.density(network):.4f}")
            st.metric("è¿é€šæ€§", "æ˜¯" if nx.is_connected(network) else "å¦")
        
        with col2:
            st.subheader("æ‹“æ‰‘æŒ‡æ ‡")
            if nx.is_connected(network):
                st.metric("ç›´å¾„", nx.diameter(network))
                st.metric("å¹³å‡è·¯å¾„é•¿åº¦", f"{nx.average_shortest_path_length(network):.2f}")
                st.metric("åŠå¾„", nx.radius(network))
            else:
                st.metric("è¿é€šåˆ†é‡æ•°", nx.number_connected_components(network))
        
        with col3:
            st.subheader("ç½‘ç»œç‰¹æ€§")
            st.metric("å¹³å‡èšç±»ç³»æ•°", f"{nx.average_clustering(network):.4f}")
            st.metric("åº¦æ•°åŒé…æ€§", f"{nx.degree_assortativity_coefficient(network):.4f}")
        
        # Detailed network visualization
        st.subheader("ğŸ—ºï¸ è¯¦ç»†ç½‘ç»œå›¾")
        
        layout_type = st.selectbox(
            "é€‰æ‹©å¸ƒå±€ç®—æ³•",
            ['spring', 'circular', 'random'],
            format_func=lambda x: {
                'spring': 'å¼¹ç°§å¸ƒå±€',
                'circular': 'ç¯å½¢å¸ƒå±€',
                'random': 'éšæœºå¸ƒå±€'
            }[x]
        )
        
        self.plot_detailed_network(layout_type)
        
        # Centrality analysis
        st.subheader("ğŸ“Š ä¸­å¿ƒæ€§åˆ†æ")
        self.plot_centrality_analysis()
    
    def plot_detailed_network(self, layout_type: str = 'spring'):
        """Plot detailed network with interactive features"""
        network = st.session_state.network
        
        # Choose layout
        if layout_type == 'spring':
            pos = nx.spring_layout(network, k=1, iterations=50)
        elif layout_type == 'circular':
            pos = nx.circular_layout(network)
        else:
            pos = nx.random_layout(network)
        
        # Calculate node metrics
        degrees = dict(network.degree())
        betweenness = nx.betweenness_centrality(network)
        closeness = nx.closeness_centrality(network)
        
        # Prepare data
        node_x = [pos[node][0] for node in network.nodes()]
        node_y = [pos[node][1] for node in network.nodes()]
        
        # Node sizes based on degree
        node_sizes = [degrees[node] * 3 + 5 for node in network.nodes()]
        
        # Node colors based on betweenness centrality
        node_colors = [betweenness[node] for node in network.nodes()]
        
        # Edge data
        edge_x = []
        edge_y = []
        for edge in network.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
        
        # Create figure
        fig = go.Figure()
        
        # Add edges
        fig.add_trace(go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=1, color='rgba(125,125,125,0.5)'),
            hoverinfo='none',
            mode='lines',
            showlegend=False
        ))
        
        # Add nodes
        hover_text = [
            f"èŠ‚ç‚¹: {node}<br>"
            f"åº¦æ•°: {degrees[node]}<br>"
            f"ä»‹æ•°ä¸­å¿ƒæ€§: {betweenness[node]:.3f}<br>"
            f"æ¥è¿‘ä¸­å¿ƒæ€§: {closeness[node]:.3f}"
            for node in network.nodes()
        ]
        
        fig.add_trace(go.Scatter(
            x=node_x, y=node_y,
            mode='markers',
            marker=dict(
                size=node_sizes,
                color=node_colors,
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(title="ä»‹æ•°ä¸­å¿ƒæ€§"),
                line=dict(width=1, color='black')
            ),
            text=hover_text,
            hoverinfo='text',
            showlegend=False
        ))
        
        fig.update_layout(
            title=f"ç½‘ç»œæ‹“æ‰‘å›¾ - {layout_type}å¸ƒå±€",
            showlegend=False,
            hovermode='closest',
            margin=dict(b=20,l=5,r=5,t=40),
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            height=600
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    def plot_centrality_analysis(self):
        """Plot centrality measures analysis"""
        network = st.session_state.network
        
        # Calculate centralities
        degree_cent = nx.degree_centrality(network)
        betweenness_cent = nx.betweenness_centrality(network)
        closeness_cent = nx.closeness_centrality(network)
        
        # Create DataFrame
        nodes = list(network.nodes())
        centrality_df = pd.DataFrame({
            'node': nodes,
            'degree': [degree_cent[node] for node in nodes],
            'betweenness': [betweenness_cent[node] for node in nodes],
            'closeness': [closeness_cent[node] for node in nodes]
        })
        
        # Create subplots
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=['åº¦æ•°ä¸­å¿ƒæ€§', 'ä»‹æ•°ä¸­å¿ƒæ€§', 'æ¥è¿‘ä¸­å¿ƒæ€§', 'ä¸­å¿ƒæ€§æ¯”è¾ƒ'],
            specs=[[{"type": "bar"}, {"type": "bar"}],
                   [{"type": "bar"}, {"type": "scatter"}]]
        )
        
        # Degree centrality
        top_degree = centrality_df.nlargest(10, 'degree')
        fig.add_trace(
            go.Bar(x=top_degree['node'], y=top_degree['degree'], name='åº¦æ•°ä¸­å¿ƒæ€§'),
            row=1, col=1
        )
        
        # Betweenness centrality
        top_between = centrality_df.nlargest(10, 'betweenness')
        fig.add_trace(
            go.Bar(x=top_between['node'], y=top_between['betweenness'], name='ä»‹æ•°ä¸­å¿ƒæ€§'),
            row=1, col=2
        )
        
        # Closeness centrality
        top_close = centrality_df.nlargest(10, 'closeness')
        fig.add_trace(
            go.Bar(x=top_close['node'], y=top_close['closeness'], name='æ¥è¿‘ä¸­å¿ƒæ€§'),
            row=2, col=1
        )
        
        # Centrality comparison scatter
        fig.add_trace(
            go.Scatter(
                x=centrality_df['degree'],
                y=centrality_df['betweenness'],
                mode='markers',
                text=centrality_df['node'],
                name='åº¦æ•° vs ä»‹æ•°',
                marker=dict(
                    size=centrality_df['closeness'] * 20 + 5,
                    color=centrality_df['closeness'],
                    colorscale='Viridis',
                    showscale=True
                )
            ),
            row=2, col=2
        )
        
        fig.update_layout(height=800, showlegend=False)
        st.plotly_chart(fig, use_container_width=True)
    
    def render_anomaly_page(self):
        """Render anomaly detection analysis page"""
        st.header("ğŸš¨ å¼‚å¸¸æ£€æµ‹åˆ†æ")
        
        if st.session_state.data is None or st.session_state.anomaly_analyzer is None:
            st.info("è¯·å…ˆç”Ÿæˆæ•°æ®å¹¶è®­ç»ƒæ¨¡å‹")
            return
        
        data = st.session_state.data
        analyzer = st.session_state.anomaly_analyzer
        
        # Anomaly detection results
        test_data = data.tail(int(len(data) * 0.3))  # Use last 30% as test
        detection_results = analyzer.detect_anomalies(test_data)
        
        # Summary metrics
        col1, col2, col3, col4 = st.columns(4)
        
        summary = detection_results['summary']
        with col1:
            st.metric("æµ‹è¯•æ ·æœ¬æ•°", summary['total_samples'])
        
        with col2:
            st.metric("é¢„æµ‹å¼‚å¸¸æ•°", summary['predicted_anomalies'])
        
        with col3:
            st.metric("å¼‚å¸¸ç‡", f"{summary['anomaly_rate']:.1%}")
        
        with col4:
            st.metric("å¹³å‡å¼‚å¸¸æ¦‚ç‡", f"{summary['mean_anomaly_probability']:.3f}")
        
        # Detailed visualization
        st.subheader("ğŸ“Š å¼‚å¸¸æ£€æµ‹ç»“æœå¯è§†åŒ–")
        
        tab1, tab2, tab3 = st.tabs(["æ—¶åºåˆ†æ", "ç‰¹å¾åˆ†å¸ƒ", "æ£€æµ‹æ€§èƒ½"])
        
        with tab1:
            self.plot_anomaly_timeseries(detection_results)
        
        with tab2:
            self.plot_feature_distributions(detection_results)
        
        with tab3:
            self.plot_detection_performance(detection_results, test_data)
    
    def plot_anomaly_timeseries(self, detection_results):
        """Plot anomaly detection time series results"""
        results_df = detection_results['results']
        
        # Create subplots
        fig = make_subplots(
            rows=3, cols=1,
            shared_xaxes=True,
            subplot_titles=['ç½‘ç»œæµé‡', 'å¼‚å¸¸æ¦‚ç‡', 'æ£€æµ‹ç»“æœ'],
            vertical_spacing=0.1
        )
        
        # Find traffic column
        traffic_col = None
        for col in ['traffic_mbps', 'traffic', 'throughput_mbps', 'network_throughput']:
            if col in results_df.columns:
                traffic_col = col
                break
        
        if traffic_col:
            # Plot traffic
            fig.add_trace(
                go.Scatter(
                    x=results_df.index,
                    y=results_df[traffic_col],
                    mode='lines',
                    name='æµé‡',
                    line=dict(color='blue')
                ),
                row=1, col=1
        )
        
        # Highlight actual anomalies in traffic plot
        if 'is_anomaly' in results_df.columns and traffic_col:
            anomaly_points = results_df[results_df['is_anomaly'] == True]
            if not anomaly_points.empty:
                fig.add_trace(
                    go.Scatter(
                        x=anomaly_points.index,
                        y=anomaly_points[traffic_col],
                        mode='markers',
                        name='çœŸå®å¼‚å¸¸',
                        marker=dict(color='red', size=6)
                    ),
                    row=1, col=1
                )
        
        # Plot anomaly probability
        fig.add_trace(
            go.Scatter(
                x=results_df.index,
                y=results_df['anomaly_probability'],
                mode='lines',
                name='å¼‚å¸¸æ¦‚ç‡',
                line=dict(color='orange'),
                fill='tozeroy'
            ),
            row=2, col=1
        )
        
        # Add threshold line
        fig.add_hline(y=0.5, line_dash="dash", line_color="red", 
                     annotation_text="é˜ˆå€¼", row=2, col=1)
        
        # Plot detection results
        predicted_anomalies = results_df[results_df['predicted_anomaly'] == True]
        if not predicted_anomalies.empty:
            fig.add_trace(
                go.Scatter(
                    x=predicted_anomalies.index,
                    y=[1] * len(predicted_anomalies),
                    mode='markers',
                    name='é¢„æµ‹å¼‚å¸¸',
                    marker=dict(color='red', size=8, symbol='x')
                ),
                row=3, col=1
            )
        
        fig.update_layout(
            height=600,
            title="å¼‚å¸¸æ£€æµ‹æ—¶åºåˆ†æ",
            showlegend=True
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    def plot_feature_distributions(self, detection_results):
        """Plot feature distributions for normal vs anomaly"""
        results_df = detection_results['results']
        
        # Select numeric features
        # Auto-detect feature columns
        feature_columns = []
        column_mappings = {
            'traffic': ['traffic_mbps', 'traffic', 'throughput_mbps', 'network_throughput'],
            'latency': ['latency_ms', 'latency', 'response_time_ms'],
            'packet_loss': ['packet_loss_rate', 'packet_loss', 'loss_rate'],
            'cpu_usage': ['cpu_usage', 'cpu_util', 'processor_usage'],
            'memory_usage': ['memory_usage', 'mem_usage', 'memory_util']
        }
        
        for standard_name, possible_names in column_mappings.items():
            for col_name in possible_names:
                if col_name in results_df.columns:
                    feature_columns.append(col_name)
                    break
        
        if not feature_columns:
            # Fallback: use numeric columns except timestamp and anomaly columns
            feature_columns = [col for col in results_df.columns 
                             if col not in ['timestamp', 'is_anomaly', 'anomaly_score'] 
                             and results_df[col].dtype in ['float64', 'int64']][:5]
        available_features = [col for col in feature_columns if col in results_df.columns]
        
        if not available_features:
            st.warning("æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾æ•°æ®")
            return
        
        # Create comparison plots
        n_features = len(available_features)
        cols = min(3, n_features)
        rows = (n_features + cols - 1) // cols
        
        fig = make_subplots(
            rows=rows, cols=cols,
            subplot_titles=available_features
        )
        
        for i, feature in enumerate(available_features):
            row = i // cols + 1
            col = i % cols + 1
            
            # Normal data
            normal_data = results_df[results_df['predicted_anomaly'] == False][feature]
            
            # Anomaly data
            anomaly_data = results_df[results_df['predicted_anomaly'] == True][feature]
            
            # Plot histograms
            fig.add_trace(
                go.Histogram(
                    x=normal_data,
                    name=f'{feature} - æ­£å¸¸',
                    opacity=0.7,
                    nbinsx=20
                ),
                row=row, col=col
            )
            
            if not anomaly_data.empty:
                fig.add_trace(
                    go.Histogram(
                        x=anomaly_data,
                        name=f'{feature} - å¼‚å¸¸',
                        opacity=0.7,
                        nbinsx=20
                    ),
                    row=row, col=col
                )
        
        fig.update_layout(
            height=400 * rows,
            title="ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”ï¼šæ­£å¸¸ vs å¼‚å¸¸",
            showlegend=True,
            barmode='overlay'
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    def plot_detection_performance(self, detection_results, test_data):
        """Plot detection performance metrics"""
        if 'is_anomaly' not in test_data.columns:
            st.warning("æ²¡æœ‰çœŸå®æ ‡ç­¾æ•°æ®ï¼Œæ— æ³•è¯„ä¼°æ€§èƒ½")
            return
        
        results_df = detection_results['results']
        
        # Calculate performance metrics
        y_true = test_data['is_anomaly'].astype(int)
        y_pred = results_df['predicted_anomaly'].astype(int)
        y_prob = results_df['anomaly_probability']
        
        # Confusion matrix
        from sklearn.metrics import confusion_matrix, classification_report
        
        cm = confusion_matrix(y_true, y_pred)
        
        # Create performance visualization
        col1, col2 = st.columns(2)
        
        with col1:
            # Confusion matrix heatmap
            fig_cm = px.imshow(
                cm,
                text_auto=True,
                aspect="auto",
                title="æ··æ·†çŸ©é˜µ",
                labels=dict(x="é¢„æµ‹", y="çœŸå®", color="æ ·æœ¬æ•°")
            )
            fig_cm.update_xaxes(tickmode='array', tickvals=[0, 1], ticktext=['æ­£å¸¸', 'å¼‚å¸¸'])
            fig_cm.update_yaxes(tickmode='array', tickvals=[0, 1], ticktext=['æ­£å¸¸', 'å¼‚å¸¸'])
            st.plotly_chart(fig_cm, use_container_width=True)
        
        with col2:
            # ROC curve (simplified)
            from sklearn.metrics import roc_curve, auc
            
            try:
                fpr, tpr, _ = roc_curve(y_true, y_prob)
                roc_auc = auc(fpr, tpr)
                
                fig_roc = go.Figure()
                fig_roc.add_trace(go.Scatter(
                    x=fpr, y=tpr,
                    mode='lines',
                    name=f'ROCæ›²çº¿ (AUC = {roc_auc:.3f})'
                ))
                fig_roc.add_trace(go.Scatter(
                    x=[0, 1], y=[0, 1],
                    mode='lines',
                    line=dict(dash='dash'),
                    name='éšæœºçŒœæµ‹'
                ))
                
                fig_roc.update_layout(
                    title='ROCæ›²çº¿',
                    xaxis_title='å‡æ­£ç‡',
                    yaxis_title='çœŸæ­£ç‡'
                )
                
                st.plotly_chart(fig_roc, use_container_width=True)
                
            except Exception as e:
                st.error(f"ROCæ›²çº¿ç»˜åˆ¶å¤±è´¥: {str(e)}")
        
        # Classification report
        st.subheader("ğŸ“‹ åˆ†ç±»æŠ¥å‘Š")
        
        report = classification_report(y_true, y_pred, target_names=['æ­£å¸¸', 'å¼‚å¸¸'], output_dict=True)
        
        report_df = pd.DataFrame(report).transpose()
        st.dataframe(report_df.round(3))
    
    def render_cascading_page(self):
        """Render cascading failure analysis page"""
        st.header("âš¡ çº§è”å¤±æ•ˆåˆ†æ")
        
        if st.session_state.network is None or st.session_state.cascade_analyzer is None:
            st.info("è¯·å…ˆç”Ÿæˆç½‘ç»œæ•°æ®")
            return
        
        network = st.session_state.network
        analyzer = st.session_state.cascade_analyzer
        
        # Analysis controls
        col1, col2 = st.columns(2)
        
        with col1:
            initial_failure_count = st.slider("åˆå§‹å¤±æ•ˆèŠ‚ç‚¹æ•°", 1, 5, 1)
        
        with col2:
            if st.button("ğŸ”¬ å¼€å§‹çº§è”å¤±æ•ˆåˆ†æ"):
                with st.spinner("æ­£åœ¨è¿›è¡Œçº§è”å¤±æ•ˆåˆ†æ..."):
                    try:
                        results = analyzer.analyze_network_robustness(network)
                        st.session_state.analysis_results['cascading'] = results
                        st.success("åˆ†æå®Œæˆï¼")
                    except Exception as e:
                        st.error(f"åˆ†æå¤±è´¥: {str(e)}")
        
        # Display results if available
        if 'cascading' in st.session_state.analysis_results:
            results = st.session_state.analysis_results['cascading']
            
            # Overall robustness metrics
            st.subheader("ğŸ“Š ç½‘ç»œé²æ£’æ€§è¯„ä¼°")
            
            robustness_metrics = results.get('robustness_metrics', {})
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if 'overall_robustness_score' in robustness_metrics:
                    score = robustness_metrics['overall_robustness_score']
                    st.metric("æ€»ä½“é²æ£’æ€§è¯„åˆ†", f"{score:.3f}")
                    
                    if score > 0.8:
                        st.success("ğŸŸ¢ ç½‘ç»œé²æ£’æ€§å¼º")
                    elif score > 0.6:
                        st.warning("ğŸŸ¡ ç½‘ç»œé²æ£’æ€§ä¸­ç­‰")
                    else:
                        st.error("ğŸ”´ ç½‘ç»œé²æ£’æ€§å¼±")
            
            with col2:
                single_failures = results.get('single_node_failures', {})
                if 'statistics' in single_failures:
                    stats = single_failures['statistics']
                    avg_cascade = stats.get('mean_final_failures', 0)
                    st.metric("å¹³å‡çº§è”è§„æ¨¡", f"{avg_cascade:.1f}")
            
            with col3:
                if 'statistics' in single_failures:
                    max_cascade = stats.get('max_final_failures', 0)
                    st.metric("æœ€å¤§çº§è”è§„æ¨¡", f"{max_cascade}")
            
            # Critical nodes analysis
            st.subheader("ğŸ¯ å…³é”®èŠ‚ç‚¹åˆ†æ")
            
            critical_nodes = results.get('critical_nodes', {})
            if 'critical_nodes_ranking' in critical_nodes:
                ranking = critical_nodes['critical_nodes_ranking'][:10]
                
                # Create bar chart
                if ranking:
                    nodes, impacts = zip(*ranking)
                    
                    fig = px.bar(
                        x=list(nodes),
                        y=list(impacts),
                        title="å…³é”®èŠ‚ç‚¹æ’åï¼ˆå‰10ä½ï¼‰",
                        labels={'x': 'èŠ‚ç‚¹ID', 'y': 'çº§è”å¤±æ•ˆå½±å“'}
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                
                # Critical nodes table
                st.subheader("ğŸ“‹ å…³é”®èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯")
                
                critical_df = pd.DataFrame(ranking, columns=['èŠ‚ç‚¹ID', 'çº§è”å½±å“'])
                st.dataframe(critical_df)
            
            # Analysis report
            st.subheader("ğŸ“„ åˆ†ææŠ¥å‘Š")
            
            report = analyzer.generate_report()
            st.text(report)
    
    def render_explainability_page(self):
        """Render explainability analysis page"""
        st.header("ğŸ” å¯è§£é‡Šæ€§åˆ†æ")
        
        if (st.session_state.data is None or 
            st.session_state.anomaly_analyzer is None or 
            st.session_state.explainer is None):
            st.info("è¯·å…ˆç”Ÿæˆæ•°æ®å¹¶è®­ç»ƒæ¨¡å‹")
            return
        
        data = st.session_state.data
        analyzer = st.session_state.anomaly_analyzer
        explainer = st.session_state.explainer
        
        st.subheader("ğŸ¯ å•ä¸ªå®ä¾‹è§£é‡Š")
        
        # Instance selection
        test_data = data.tail(100)  # Use last 100 samples
        
        instance_idx = st.selectbox(
            "é€‰æ‹©è¦è§£é‡Šçš„å®ä¾‹",
            range(len(test_data)),
            format_func=lambda x: f"å®ä¾‹ {x} ({'å¼‚å¸¸' if test_data.iloc[x]['is_anomaly'] else 'æ­£å¸¸'})"
        )
        
        selected_instance = test_data.iloc[instance_idx]
        
        # Show instance details
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("å®ä¾‹è¯¦æƒ…")
            # Auto-detect feature columns 
            feature_cols = []
            column_mappings = {
                'traffic': ['traffic_mbps', 'traffic', 'throughput_mbps'],
                'latency': ['latency_ms', 'latency', 'response_time_ms'],
                'packet_loss': ['packet_loss_rate', 'packet_loss'],
                'cpu_usage': ['cpu_usage', 'cpu_util'],
                'memory_usage': ['memory_usage', 'mem_usage']
            }
            
            for standard_name, possible_names in column_mappings.items():
                for col_name in possible_names:
                    if col_name in data.columns:
                        feature_cols.append(col_name)
                        break
                        
            if not feature_cols:
                # Fallback: use numeric columns
                feature_cols = [col for col in data.columns 
                               if col not in ['timestamp', 'is_anomaly', 'anomaly_score'] 
                               and data[col].dtype in ['float64', 'int64']][:5]
            available_features = [col for col in feature_cols if col in selected_instance.index]
            
            for feature in available_features:
                st.metric(feature, f"{selected_instance[feature]:.3f}")
        
        with col2:
            st.subheader("æ£€æµ‹ç»“æœ")
            st.metric("çœŸå®æ ‡ç­¾", "å¼‚å¸¸" if selected_instance['is_anomaly'] else "æ­£å¸¸")
            
            # Get prediction for this instance
            X_instance = analyzer.prepare_features(
                pd.DataFrame([selected_instance]), 
                analyzer.feature_columns
            )
            
            detection_result = analyzer.detect_anomalies(pd.DataFrame([selected_instance]))
            prediction = detection_result['results']['predicted_anomaly'].iloc[0]
            probability = detection_result['results']['anomaly_probability'].iloc[0]
            
            st.metric("é¢„æµ‹æ ‡ç­¾", "å¼‚å¸¸" if prediction else "æ­£å¸¸")
            st.metric("å¼‚å¸¸æ¦‚ç‡", f"{probability:.3f}")
        
        # Generate explanation
        if st.button("ğŸ” ç”Ÿæˆè§£é‡Š"):
            with st.spinner("æ­£åœ¨ç”Ÿæˆè§£é‡Š..."):
                try:
                    explanation = explainer.explain_instance(X_instance[0], f"instance_{instance_idx}")
                    
                    # Display explanation
                    st.subheader("ğŸ“‹ è§£é‡Šç»“æœ")
                    
                    # Explanation text
                    st.write("**è§£é‡Šæ–‡æœ¬:**")
                    st.write(explanation['explanation_text'])
                    
                    # Feature contributions
                    st.subheader("ğŸ“Š ç‰¹å¾è´¡çŒ®åº¦")
                    
                    # Create feature importance plot
                    top_features = explanation['top_features']
                    if top_features:
                        features, importances = zip(*top_features)
                        
                        # Get SHAP values for coloring
                        shap_values = [explanation['shap_values'][f] for f in features]
                        colors = ['red' if v > 0 else 'blue' for v in shap_values]
                        
                        fig = go.Figure(data=[
                            go.Bar(
                                x=list(features),
                                y=list(importances),
                                marker_color=colors,
                                text=[f'{v:.3f}' for v in shap_values],
                                textposition='outside'
                            )
                        ])
                        
                        fig.update_layout(
                            title="ç‰¹å¾é‡è¦æ€§ï¼ˆSHAPå€¼ï¼‰",
                            xaxis_title="ç‰¹å¾",
                            yaxis_title="é‡è¦æ€§",
                            height=400
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
                    
                    # Confidence indicators
                    st.subheader("ğŸ¯ è§£é‡Šç½®ä¿¡åº¦")
                    
                    confidence = explanation['confidence_indicators']
                    
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric("ä¸€è‡´æ€§", f"{confidence['consistency']:.3f}")
                    
                    with col2:
                        st.metric("é›†ä¸­åº¦", f"{confidence['concentration']:.3f}")
                    
                    with col3:
                        st.metric("ç¨³å®šæ€§", f"{confidence['stability']:.3f}")
                    
                    with col4:
                        overall_conf = confidence['overall_confidence']
                        st.metric("æ€»ä½“ç½®ä¿¡åº¦", f"{overall_conf:.3f}")
                        
                        if overall_conf > 0.8:
                            st.success("ğŸŸ¢ é«˜ç½®ä¿¡åº¦è§£é‡Š")
                        elif overall_conf > 0.6:
                            st.warning("ğŸŸ¡ ä¸­ç­‰ç½®ä¿¡åº¦è§£é‡Š")
                        else:
                            st.error("ğŸ”´ ä½ç½®ä¿¡åº¦è§£é‡Š")
                
                except Exception as e:
                    st.error(f"è§£é‡Šç”Ÿæˆå¤±è´¥: {str(e)}")

    def render_ai_analysis_page(self):
        """Render AI model analysis page"""
        st.header("ğŸ¤– AIå¤§æ¨¡å‹åˆ†æ")

        if st.session_state.network is None or st.session_state.data is None:
            st.info("ğŸ‘† è¯·å…ˆåœ¨ä¾§è¾¹æ ç”Ÿæˆç½‘ç»œå’Œæ•°æ®")
            return

        # æ™ºèƒ½æ‘˜è¦å’Œå¯¹è¯ç•Œé¢
        col1, col2 = st.columns([1, 2])

        with col1:
            st.subheader("ğŸ“Š æ™ºèƒ½æ‘˜è¦")
            self.render_ai_summary_panel()

        with col2:
            st.subheader("ğŸ’¬ AIåˆ†æåŠ©æ‰‹")
            self.render_ai_chat_interface()

        # ä¸‹æ–¹åˆ†ææ¨¡å—
        st.markdown("---")

        tab1, tab2, tab3 = st.tabs(["ğŸ“ˆ è¶‹åŠ¿åˆ†æ", "ğŸ” æ¨¡å¼è¯†åˆ«", "âš ï¸ é£é™©è¯„ä¼°"])

        with tab1:
            self.render_ai_trend_analysis()

        with tab2:
            self.render_ai_pattern_recognition()

        with tab3:
            self.render_ai_risk_assessment()

    def render_ai_summary_panel(self):
        """æ¸²æŸ“AIæ™ºèƒ½æ‘˜è¦é¢æ¿"""
        if st.button("ğŸ”„ ç”Ÿæˆæ™ºèƒ½æ‘˜è¦", type="primary"):
            with st.spinner("æ­£åœ¨ç”Ÿæˆæ™ºèƒ½æ‘˜è¦..."):
                context = self.get_system_context()

                system_prompt = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘ç»œå®‰å…¨åˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹ç³»ç»ŸçŠ¶æ€æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„æ™ºèƒ½æ‘˜è¦æŠ¥å‘Šï¼š

                ç½‘ç»œçŠ¶æ€ï¼š{context.get('network', {})}
                æ•°æ®çŠ¶æ€ï¼š{context.get('data', {})}
                å‘Šè­¦çŠ¶æ€ï¼š{context.get('alerts', {})}

                è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œæ€»ç»“ï¼š
                1. ç½‘ç»œæ•´ä½“å¥åº·çŠ¶å†µ
                2. å½“å‰ä¸»è¦é£é™©ç‚¹
                3. éœ€è¦å…³æ³¨çš„å¼‚å¸¸æƒ…å†µ
                4. ç®€è¦çš„å»ºè®®æªæ–½

                è¦æ±‚ç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹ï¼Œæ§åˆ¶åœ¨300å­—ä»¥å†…ã€‚
                """

                messages = [{"role": "user", "content": "è¯·ç”Ÿæˆå½“å‰ç³»ç»Ÿçš„æ™ºèƒ½æ‘˜è¦æŠ¥å‘Š"}]

                response = self.call_zhipu_ai(messages, system_prompt)
                st.session_state.ai_summary = response

        if st.session_state.ai_summary:
            st.markdown("### ğŸ“‹ ç³»ç»Ÿæ‘˜è¦")
            st.markdown(st.session_state.ai_summary)
        else:
            st.info("ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®ç”Ÿæˆæ™ºèƒ½æ‘˜è¦")

    def render_ai_chat_interface(self):
        """æ¸²æŸ“AIå¯¹è¯ç•Œé¢"""
        # é¢„è®¾é—®é¢˜
        st.markdown("### ğŸš€ å¿«é€Ÿé—®é¢˜")

        quick_questions = [
            "å½“å‰ç½‘ç»œæ•´ä½“å¥åº·çŠ¶å†µå¦‚ä½•ï¼Ÿ",
            "æœ‰å“ªäº›éœ€è¦ç«‹å³å…³æ³¨çš„é—®é¢˜ï¼Ÿ",
            "è§£é‡Šæœ€æ–°æ£€æµ‹åˆ°çš„å¼‚å¸¸åŸå› ",
            "ç½‘ç»œçš„çº§è”å¤±æ•ˆé£é™©æœ‰å¤šé«˜ï¼Ÿ",
            "å¦‚ä½•æå‡ç½‘ç»œçš„é²æ£’æ€§ï¼Ÿ"
        ]

        col1, col2 = st.columns(2)
        for i, question in enumerate(quick_questions):
            col = col1 if i % 2 == 0 else col2
            with col:
                if st.button(question, key=f"quick_q_{i}"):
                    st.session_state.ai_chat_history.append({"role": "user", "content": question})
                    with st.spinner("AIæ­£åœ¨æ€è€ƒ..."):
                        context = self.get_system_context()
                        system_prompt = f"""
                        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘ç»œå®‰å…¨åˆ†æå¸ˆå’Œå¤æ‚ç½‘ç»œä¸“å®¶ã€‚å½“å‰ç³»ç»ŸçŠ¶æ€ï¼š

                        ç½‘ç»œçŠ¶æ€ï¼š{context.get('network', {})}
                        æ•°æ®çŠ¶æ€ï¼š{context.get('data', {})}
                        å‘Šè­¦çŠ¶æ€ï¼š{context.get('alerts', {})}

                        è¯·åŸºäºå®é™…æ•°æ®å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œæä¾›ä¸“ä¸šã€å‡†ç¡®ã€å¯æ“ä½œçš„å»ºè®®ã€‚
                        """

                        messages = [{"role": "user", "content": question}]
                        response = self.call_zhipu_ai(messages, system_prompt)
                        st.session_state.ai_chat_history.append({"role": "assistant", "content": response})
                    st.rerun()

        # å¯¹è¯å†å²
        st.markdown("### ğŸ’¬ å¯¹è¯å†å²")

        chat_container = st.container()
        with chat_container:
            for i, message in enumerate(st.session_state.ai_chat_history[-6:]):  # æ˜¾ç¤ºæœ€è¿‘6æ¡
                if message["role"] == "user":
                    st.markdown(f"**ğŸ‘¤ ç”¨æˆ·ï¼š** {message['content']}")
                else:
                    st.markdown(f"**ğŸ¤– AIåŠ©æ‰‹ï¼š** {message['content']}")
                st.markdown("---")

        # è‡ªå®šä¹‰é—®é¢˜è¾“å…¥
        with st.form("ai_chat_form", clear_on_submit=True):
            user_input = st.text_area("è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š", placeholder="ä¾‹å¦‚ï¼šåˆ†æå½“å‰ç½‘ç»œçš„ä¸»è¦é£é™©ç‚¹", height=100)
            submitted = st.form_submit_button("å‘é€", type="primary")

            if submitted and user_input:
                st.session_state.ai_chat_history.append({"role": "user", "content": user_input})

                with st.spinner("AIæ­£åœ¨åˆ†æ..."):
                    context = self.get_system_context()
                    system_prompt = f"""
                    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘ç»œå®‰å…¨åˆ†æå¸ˆå’Œå¤æ‚ç½‘ç»œä¸“å®¶ã€‚å½“å‰ç³»ç»ŸçŠ¶æ€ï¼š

                    ç½‘ç»œçŠ¶æ€ï¼š{context.get('network', {})}
                    æ•°æ®çŠ¶æ€ï¼š{context.get('data', {})}
                    å‘Šè­¦çŠ¶æ€ï¼š{context.get('alerts', {})}

                    è¯·åŸºäºå®é™…æ•°æ®å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œæä¾›ä¸“ä¸šã€å‡†ç¡®ã€å¯æ“ä½œçš„å»ºè®®ã€‚ä¿æŒå›ç­”ç®€æ´æ˜äº†ã€‚
                    """

                    # è·å–æœ€è¿‘çš„å¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡
                    recent_messages = st.session_state.ai_chat_history[-4:]  # æœ€è¿‘4æ¡æ¶ˆæ¯

                    response = self.call_zhipu_ai(recent_messages, system_prompt)
                    st.session_state.ai_chat_history.append({"role": "assistant", "content": response})

                st.rerun()

    def render_ai_trend_analysis(self):
        """æ¸²æŸ“AIè¶‹åŠ¿åˆ†æ"""
        st.markdown("### ğŸ“ˆ æ™ºèƒ½è¶‹åŠ¿åˆ†æ")

        if st.button("ğŸ” åˆ†ææ•°æ®è¶‹åŠ¿"):
            if st.session_state.data is not None:
                with st.spinner("æ­£åœ¨è¿›è¡Œè¶‹åŠ¿åˆ†æ..."):
                    data = st.session_state.data

                    # è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
                    recent_data = data.tail(100)
                    stats_info = {
                        "å¼‚å¸¸ç‡è¶‹åŠ¿": recent_data['is_anomaly'].rolling(window=10).mean().iloc[-1] if 'is_anomaly' in data.columns else 0,
                        "æ•°æ®å˜åŒ–": len(recent_data),
                        "ç‰¹å¾æ•°é‡": len([col for col in data.columns if col not in ['timestamp', 'is_anomaly', 'anomaly_score']])
                    }

                    system_prompt = f"""
                    åŸºäºç½‘ç»œç›‘æ§æ•°æ®ï¼Œè¿›è¡Œè¶‹åŠ¿åˆ†æã€‚å½“å‰æ•°æ®ç»Ÿè®¡ï¼š{stats_info}

                    è¯·ä»ä»¥ä¸‹è§’åº¦åˆ†æï¼š
                    1. å¼‚å¸¸è¶‹åŠ¿å˜åŒ–
                    2. å¯èƒ½çš„å‘¨æœŸæ€§æ¨¡å¼
                    3. é£é™©é¢„è­¦ä¿¡å·
                    4. ä¼˜åŒ–å»ºè®®
                    """

                    messages = [{"role": "user", "content": "è¯·å¯¹å½“å‰ç½‘ç»œæ•°æ®è¿›è¡Œè¶‹åŠ¿åˆ†æ"}]
                    response = self.call_zhipu_ai(messages, system_prompt)

                    st.markdown(response)
            else:
                st.warning("æš‚æ— æ•°æ®ç”¨äºè¶‹åŠ¿åˆ†æ")

    def render_ai_pattern_recognition(self):
        """æ¸²æŸ“AIæ¨¡å¼è¯†åˆ«"""
        st.markdown("### ğŸ” æ™ºèƒ½æ¨¡å¼è¯†åˆ«")

        if st.button("ğŸ¯ è¯†åˆ«å¼‚å¸¸æ¨¡å¼"):
            if st.session_state.data is not None and st.session_state.anomaly_analyzer is not None:
                with st.spinner("æ­£åœ¨è¯†åˆ«å¼‚å¸¸æ¨¡å¼..."):
                    data = st.session_state.data
                    anomaly_data = data[data['is_anomaly'] == True] if 'is_anomaly' in data.columns else pd.DataFrame()

                    pattern_info = {
                        "å¼‚å¸¸æ•°é‡": len(anomaly_data),
                        "å¼‚å¸¸ç‡": data['is_anomaly'].mean() if 'is_anomaly' in data.columns else 0,
                        "ç‰¹å¾ç›¸å…³æ€§": "å·²åˆ†æ" if st.session_state.anomaly_analyzer else "æœªåˆ†æ"
                    }

                    system_prompt = f"""
                    åŸºäºç½‘ç»œå¼‚å¸¸æ£€æµ‹ç»“æœï¼Œè¯†åˆ«å¼‚å¸¸è¡Œä¸ºæ¨¡å¼ã€‚å½“å‰å¼‚å¸¸ç»Ÿè®¡ï¼š{pattern_info}

                    è¯·åˆ†æï¼š
                    1. å¼‚å¸¸è¡Œä¸ºçš„ä¸»è¦ç‰¹å¾
                    2. å¯èƒ½çš„æ”»å‡»æ¨¡å¼æˆ–æ•…éšœç±»å‹
                    3. å¼‚å¸¸çš„åˆ†å¸ƒè§„å¾‹
                    4. é¢„é˜²æªæ–½å»ºè®®
                    """

                    messages = [{"role": "user", "content": "è¯·è¯†åˆ«å’Œåˆ†æå½“å‰ç½‘ç»œä¸­çš„å¼‚å¸¸æ¨¡å¼"}]
                    response = self.call_zhipu_ai(messages, system_prompt)

                    st.markdown(response)
            else:
                st.warning("éœ€è¦å…ˆè®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹")

    def render_ai_risk_assessment(self):
        """æ¸²æŸ“AIé£é™©è¯„ä¼°"""
        st.markdown("### âš ï¸ æ™ºèƒ½é£é™©è¯„ä¼°")

        if st.button("ğŸ“Š ç”Ÿæˆé£é™©è¯„ä¼°æŠ¥å‘Š"):
            with st.spinner("æ­£åœ¨è¯„ä¼°é£é™©..."):
                context = self.get_system_context()

                # è®¡ç®—é£é™©æŒ‡æ ‡
                risk_factors = []
                if context.get('data', {}).get('anomaly_rate', 0) > 0.1:
                    risk_factors.append("å¼‚å¸¸ç‡åé«˜")
                if context.get('alerts', {}).get('active_alerts', 0) > 0:
                    risk_factors.append("å­˜åœ¨æ´»è·ƒå‘Šè­¦")
                if not context.get('network', {}).get('is_connected', True):
                    risk_factors.append("ç½‘ç»œè¿é€šæ€§é—®é¢˜")

                system_prompt = f"""
                åŸºäºç³»ç»Ÿå½“å‰çŠ¶æ€è¿›è¡Œç»¼åˆé£é™©è¯„ä¼°ï¼š

                ç³»ç»ŸçŠ¶æ€ï¼š{context}
                é£é™©å› ç´ ï¼š{risk_factors}

                è¯·æä¾›ï¼š
                1. é£é™©ç­‰çº§è¯„ä¼°ï¼ˆä½/ä¸­/é«˜ï¼‰
                2. ä¸»è¦é£é™©å› ç´ åˆ†æ
                3. æ½œåœ¨å½±å“è¯„ä¼°
                4. é£é™©ç¼“è§£å»ºè®®
                5. ç›‘æ§é‡ç‚¹å»ºè®®
                """

                messages = [{"role": "user", "content": "è¯·å¯¹å½“å‰ç½‘ç»œç³»ç»Ÿè¿›è¡Œå…¨é¢çš„é£é™©è¯„ä¼°"}]
                response = self.call_zhipu_ai(messages, system_prompt)

                st.markdown(response)

    def render_alerts_page(self):
        """Render alerts management page"""
        st.header("ğŸ“¢ å‘Šè­¦ç®¡ç†")
        
        alert_manager = st.session_state.alert_manager
        
        # Alert statistics
        stats = alert_manager.get_alert_statistics()
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("æ€»å‘Šè­¦æ•°", stats['total_alerts'])
        
        with col2:
            st.metric("æ´»è·ƒå‘Šè­¦", stats['active_alerts'])
        
        with col3:
            st.metric("å·²è§£å†³å‘Šè­¦", stats['resolved_alerts'])
        
        with col4:
            avg_resolution = stats.get('average_resolution_time', 0) / 60
            st.metric("å¹³å‡è§£å†³æ—¶é—´", f"{avg_resolution:.1f}åˆ†é’Ÿ")
        
        # Create test alert
        st.subheader("ğŸ§ª æµ‹è¯•å‘Šè­¦ç”Ÿæˆ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ç”Ÿæˆå¼‚å¸¸æ£€æµ‹å‘Šè­¦"):
                asyncio.run(alert_manager.evaluate_rules({
                    'is_anomaly': True,
                    'confidence': 0.85,
                    'node_id': 'test_node',
                    'top_features': ['traffic_mbps', 'latency_ms']
                }))
                st.success("å¼‚å¸¸æ£€æµ‹å‘Šè­¦å·²ç”Ÿæˆ")
        
        with col2:
            if st.button("ç”Ÿæˆçº§è”å¤±æ•ˆå‘Šè­¦"):
                asyncio.run(alert_manager.evaluate_rules({
                    'failure_ratio': 0.25,
                    'failed_nodes': 5,
                    'total_nodes': 20,
                    'iterations': 3
                }))
                st.success("çº§è”å¤±æ•ˆå‘Šè­¦å·²ç”Ÿæˆ")
        
        # Active alerts
        st.subheader("ğŸš¨ å½“å‰æ´»è·ƒå‘Šè­¦")
        
        active_alerts = alert_manager.get_active_alerts()
        
        if active_alerts:
            for alert in active_alerts:
                alert_class = f"alert-{alert.level.value}"
                
                st.markdown(f"""
                <div class="{alert_class}">
                    <h4>ğŸš¨ {alert.title}</h4>
                    <p><strong>çº§åˆ«:</strong> {alert.level.value.upper()}</p>
                    <p><strong>æ—¶é—´:</strong> {alert.timestamp.strftime('%Y-%m-%d %H:%M:%S')}</p>
                    <p><strong>æè¿°:</strong> {alert.description}</p>
                    <p><strong>æ¥æº:</strong> {alert.source}</p>
                </div>
                """, unsafe_allow_html=True)
                
                # Alert actions
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button(f"ç¡®è®¤å‘Šè­¦", key=f"ack_{alert.id}"):
                        alert_manager.acknowledge_alert(alert.id, "dashboard_user")
                        st.success("å‘Šè­¦å·²ç¡®è®¤")
                        st.rerun()
                
                with col2:
                    if st.button(f"è§£å†³å‘Šè­¦", key=f"resolve_{alert.id}"):
                        alert_manager.resolve_alert(alert.id)
                        st.success("å‘Šè­¦å·²è§£å†³")
                        st.rerun()
        else:
            st.info("ğŸ‰ æš‚æ— æ´»è·ƒå‘Šè­¦")
        
        # Alert history and statistics
        st.subheader("ğŸ“Š å‘Šè­¦ç»Ÿè®¡")
        
        if stats['by_level']:
            # Alert level distribution
            levels = list(stats['by_level'].keys())
            counts = list(stats['by_level'].values())
            
            fig = px.pie(
                values=counts,
                names=levels,
                title="å‘Šè­¦çº§åˆ«åˆ†å¸ƒ"
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        # Generate alert report
        st.subheader("ğŸ“„ å‘Šè­¦æŠ¥å‘Š")
        
        if st.button("ç”Ÿæˆå‘Šè­¦æŠ¥å‘Š"):
            report = alert_manager.generate_alert_report()
            st.text(report)


def main():
    """Main application entry point"""
    dashboard = NetworkDashboard()
    dashboard.render_dashboard()


if __name__ == "__main__":
    main()